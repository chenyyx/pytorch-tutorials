{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、PyTorch Bascis\n",
    "\n",
    "上一节我们介绍了怎么安装 PyTorch，在深入介绍 PyTorch 之前，本节将先介绍一些 PyTorch 的基础知识，使得读者能够对 PyTorch 有一个大致的了解。\n",
    "\n",
    "\n",
    "## 1.1、Tensor\n",
    "\n",
    "Tensor 是 PyTorch 中重要的数据结构，是 张量 的英文，可以认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。PyTorch 里面处理的单位就是一个一个的 tensor 。 Tensor 和 NumPy 的 ndarray 类似，但是 Tensor 可以使用 GPU 进行加速。Tensor 的使用和 NumPy 以及 Matlab 的接口十分相似，下面我们通过几个例子来看看 Tensor 的基本使用。\n",
    "\n",
    "### 1.1.1、Tensor 基本用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-42 *\n",
       "       [[ 0.2060,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  1.4069,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 torch 库\n",
    "import torch as t\n",
    "\n",
    "# 构建 5x3 矩阵，这时候只是分配了空间，但是未初始化\n",
    "x = t.Tensor(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7261,  0.6628,  0.6221],\n",
       "        [ 0.8366,  0.8685,  0.9392],\n",
       "        [ 0.5753,  0.3848,  0.1666],\n",
       "        [ 0.8878,  0.1466,  0.2254],\n",
       "        [ 0.0756,  0.4799,  0.1177]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 [0, 1] 均匀分布随机初始化二维数组\n",
    "x = t.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 查看 x 的形状\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 查看列的个数，下面的两种写法是等价的\n",
    "print(x.size()[1])\n",
    "print(x.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.size 是 tuple 对象的子类，因此它支持 tuple 的所有操作，如 x.size()[0]\n",
    "\n",
    "tensor 的加法有三种不同的写法，下面总结一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最初y\n",
      "tensor([[ 0.9780,  0.1765,  0.0100],\n",
      "        [ 0.0436,  0.1372,  0.1617],\n",
      "        [ 0.3394,  0.5091,  0.5517],\n",
      "        [ 0.4899,  0.7167,  0.8781],\n",
      "        [ 0.4250,  0.0023,  0.7044]])\n",
      "第一种加法，y的结果\n",
      "tensor([[ 0.9780,  0.1765,  0.0100],\n",
      "        [ 0.0436,  0.1372,  0.1617],\n",
      "        [ 0.3394,  0.5091,  0.5517],\n",
      "        [ 0.4899,  0.7167,  0.8781],\n",
      "        [ 0.4250,  0.0023,  0.7044]])\n",
      "第二种加法，y的结果\n",
      "tensor([[ 0.9780,  0.1765,  0.0100],\n",
      "        [ 0.0436,  0.1372,  0.1617],\n",
      "        [ 0.3394,  0.5091,  0.5517],\n",
      "        [ 0.4899,  0.7167,  0.8781],\n",
      "        [ 0.4250,  0.0023,  0.7044]])\n",
      "第三种加法，y的结果\n",
      "tensor([[ 1.7041,  0.8393,  0.6321],\n",
      "        [ 0.8802,  1.0057,  1.1009],\n",
      "        [ 0.9147,  0.8939,  0.7183],\n",
      "        [ 1.3777,  0.8633,  1.1035],\n",
      "        [ 0.5006,  0.4822,  0.8221]])\n"
     ]
    }
   ],
   "source": [
    "y = t.rand(5, 3)\n",
    "\n",
    "print('最初y')\n",
    "print(y)\n",
    "\n",
    "print('第一种加法，y的结果')\n",
    "x + y # 普通加法，不改变y的内容\n",
    "print(y)\n",
    "\n",
    "print('第二种加法，y的结果')\n",
    "y.add(x) # 普通加法，不改变y的内容\n",
    "print(y)\n",
    "# 第二种写法，还可以指定加法结果的输出目标为 result\n",
    "# result = t.Tensor(5, 3)\n",
    "# t.add(x, y, out=result) # 输入到 result\n",
    "# result\n",
    "\n",
    "print('第三种加法，y的结果')\n",
    "y.add_(x) # inplace 加法，y变了\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：函数名后面带下划线 _ 的函数会修改 Tensor 本身。例如， x.add_(y) 和 x.t_() 会改变 x ，但 x.add(y) 和 x.t() 返回一个新的 Tensor ，而 x 不变。\n",
    "\n",
    "### 1.1.2、把 Tensor 当做 NumPy 使用\n",
    "\n",
    "Tensor 支持很多操作，包括数学运算、线性代数、选择、切片等等，其接口设计与 NumPy 极为相似。并且 Tensor 和 NumPy 的数组之间的互操作非常容易且快速。这就为我们提供了一种解题思路 —— 对于 Tensor 不支持的操作，可以先转为 NumPy 数组处理，之后再转回 Tensor 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入 NumPy 库\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6628,  0.8685,  0.3848,  0.1466,  0.4799])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor 的选取操作与 Numpy 类似\n",
    "x[:, 1] # 选取第二列的所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个全为 1 的 Tensor\n",
    "a = t.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 Tensor 转化为 numpy 的 ndarray，使用的是 tensor.numpy() 方法\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n",
      "tensor([ 1.,  1.,  1.,  1.,  1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 同样，我们也可以将 numpy 的ndarray 转化为 tensor\n",
    "a = np.ones(5)\n",
    "b = t.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tensor 和 NumPy 对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。\n",
    "\n",
    "下面我们总结一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 创建一个 numpy ndarray\n",
    "numpy_tensor = np.random.randn(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用下面两种方式将 numpy 的 ndarray 转换到 tensor 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pytorch_tensor1 = t.Tensor(numpy_tensor)\n",
    "pytorch_tensor2 = t.from_numpy(numpy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用以上两种方法进行转换的时候，会直接将 NumPy ndarray 的数据类型转换为对应的 PyTorch Tensor 数据类型\n",
    "\n",
    "同时我们也可以使用下面的方法将 PyTorch tensor 转换为 numpy ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor 可通过 .cuda 方法转为 GPU 的 Tensor ，从而享受 GPU 带来的加速运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 如果 pytorch tensor 在 cpu 上\n",
    "numpy_array = pytorch_tensor1.numpy()\n",
    "\n",
    "# 如果 pytorch tensor 在 gpu 上\n",
    "# numpy_array = pytorch_tensor1.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，GPU 上的 Tensor 不能直接转换为 NumPy ndarray ，需要使用 .cpu() 先将 GPU 上的 tensor 转到 CPU 上，类型跟之前保持一致\n",
    "\n",
    "PyTorch Tensor 使用 GPU 加速\n",
    "\n",
    "我们可以使用以下两种方式将 Tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 第一种方式是定义 cuda 数据类型\n",
    "# dtype = t.cuda.FloatTensor # 定义默认的 GPU 的数据类型\n",
    "# gpu_tensor = torch.randn(10, 20).type(dtype)\n",
    "\n",
    "# 第二种方式更简单，推荐使用\n",
    "# gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上\n",
    "# gpu_tensor = torch.randn(10, 20).cuda(1) # 将 tensor 放到第二个 GPU 上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用第一种方式将 tensor 放到 GPU 上的时候会将数据类型转换成定义的类型,而使用第二种方式能够直接将 tensor 放到 GPU 上,类型跟之前保持一致\n",
    "\n",
    "推荐在定义 tensor 的时候就明确数据类型，然后直接使用第二种方法将 tensor 放到 GPU 上\n",
    "\n",
    "而将 tensor 放回 CPU 的操作非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cpu_tensor = gpu_tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们能够访问到 Tensor 的一些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20])\n",
      "torch.Size([10, 20])\n"
     ]
    }
   ],
   "source": [
    "# 可以通过下面两种方式得到 tensor 的大小\n",
    "print(pytorch_tensor1.shape)\n",
    "print(pytorch_tensor1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 得到 tensor 的数据类型\n",
    "print(pytorch_tensor1.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 得到 tensor 的维度\n",
    "print(pytorch_tensor1.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 得到 tensor 的所有元素个数\n",
    "print(pytorch_tensor1.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小练习\n",
    "\n",
    "根据我们上面的介绍，以及连接 https://pytorch.org/docs/stable/tensors.html 了解 tensor 的数据类型，创建一个 float64、大小是 3 x 2、随机初始化的 tensor ，将其转化为 numpy 的 ndarray ，输出其数据类型\n",
    "\n",
    "参考输出： float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# 答案\n",
    "x = t.randn(3, 2)\n",
    "x = x.type(t.DoubleTensor)\n",
    "x_array = x.numpy()\n",
    "print(x_array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3、Tensor 的操作\n",
    "\n",
    "Tensor 操作中的 api 和 NumPy 非常相似，如果你熟悉 NumPy 中的操作，那么 tensor 基本是一致的，下面我们来列举其中的一些操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = t.ones(2, 2)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1],\n",
      "        [ 1,  1]])\n"
     ]
    }
   ],
   "source": [
    "# 将其转化为整形\n",
    "tensor1 = tensor1.long()\n",
    "# tensor1 = tensor1.type(t.LongTensor)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再将其转回 float\n",
    "tensor1 = tensor1.float()\n",
    "# tensor1 = tensor1.type(t.FloatTensor)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3999,  0.0942, -0.1311],\n",
      "        [-0.9040,  0.4237, -0.2105],\n",
      "        [-0.9978,  0.3036, -1.4675],\n",
      "        [ 1.4417, -1.1351,  0.0788]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个新的 tensor\n",
    "tensor2 = t.randn(4, 3)\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0942,  0.4237,  0.3036,  1.4417])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿着行取最大值\n",
    "max_value, max_idx = t.max(tensor2, dim=1)\n",
    "# 每一行的最大值\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  1,  1,  0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每一行最大值的下标\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4369, -0.6907, -2.1616,  0.3854])\n"
     ]
    }
   ],
   "source": [
    "# 沿着行对 tensor2 求和\n",
    "sum_x = t.sum(tensor2, dim=1)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 增加维度或者减少维度\n",
    "print(tensor2.shape)\n",
    "tensor2 = tensor2.unsqueeze(0) # 在第一维增加\n",
    "print(tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = tensor2.unsqueeze(1) # 在第二维增加\n",
    "print(tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = tensor2.squeeze(0) # 减少第一维\n",
    "print(tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor2 = tensor2.squeeze() # 将 tensor 中所有的一维全部都去掉\n",
    "print(tensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n",
      "torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor3 = t.randn(3, 4, 5)\n",
    "print(tensor3.shape)\n",
    "\n",
    "# 使用 permute 和 transpose 进行维度交换\n",
    "tensor3 = tensor3.permute(1, 0, 2) # permute 可以重新排列 tensor 的维度\n",
    "print(tensor3.shape)\n",
    "\n",
    "tensor3 = tensor3.transpose(0, 2) # transpose 交换 tensor 中的两个维度\n",
    "print(tensor3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 使用 view 对 tensor 进行 reshape\n",
    "tensor4 = t.randn(3, 4, 5)\n",
    "print(tensor4.shape)\n",
    "\n",
    "tensor4 = tensor4.view(-1, 5) # -1 表示任意的大小， 5 表示第二维变成 5\n",
    "print(tensor4.shape)\n",
    "\n",
    "tensor4 = tensor4.view(3, 20) # 重新 reshape 成 (3, 20) 的大小\n",
    "print(tensor4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor5 = t.randn(3, 4)\n",
    "tensor6 = t.randn(3, 4)\n",
    "\n",
    "# 两个 tensor 求和\n",
    "tensor7 = tensor5 + tensor6\n",
    "# z = t.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，pytorch 中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不是另外开辟内存空间，方式非常简单，一般都是在操作的符号后面加 _ ，比如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([1, 3, 3])\n",
      "torch.Size([3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor8 = t.ones(3, 3)\n",
    "print(tensor8.shape)\n",
    "\n",
    "# unsqueeze 进行 inplace\n",
    "tensor8.unsqueeze_(0)\n",
    "print(tensor8.shape)\n",
    "\n",
    "# transpose 进行 inplace\n",
    "tensor8.transpose_(1, 0)\n",
    "print(tensor8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "tensor([[ 2.,  2.,  2.],\n",
      "        [ 2.,  2.,  2.],\n",
      "        [ 2.,  2.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = t.ones(3, 3)\n",
    "t2 = t.ones(3, 3)\n",
    "print(t1)\n",
    "\n",
    "# add 进行 inplace\n",
    "t1.add_(t2)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小练习\n",
    "\n",
    "查看 https://pytorch.org/docs/stable/tensors.html 了解 tensor 更多的 api ，实现下面的要求\n",
    "\n",
    "创建一个 float32、 4x4 的全为 1 的矩阵，将矩阵正中间 2x2 的矩阵，全部修改成 2\n",
    "\n",
    "参考输出\n",
    "\n",
    "![](img/mini_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2、Variable\n",
    "\n",
    "tensor 是 PyTorch 中的完美组件，但是构建神经网络还远远不够，我们需要能够构建计算图的 tensor，这就是 Variable 。autograd.Variable 是 Autograd 中的核心类，它简单封装了Tensor，并支持几乎所有 Tensor 有的操作。Tensor在被封装为 Variable 之后，可以调用它的 .backward 实现反向传播，自动计算所有梯度。\n",
    "\n",
    "Variable 主要包含三个属性：\n",
    "\n",
    " - data ：保存 Variable 所包含的 Tensor 。\n",
    " - grad ：保存 data 对应的梯度， grad 也是个 Variable，而不是 Tensor，它和 data 的形状一样。\n",
    " - grad_fn ：指向一个 Function 对象，这个 Function 用来反向传播计算输入的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过下面这种方式导入 Variable\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tensor = t.randn(10, 5)\n",
    "y_tensor = t.randn(10, 5)\n",
    "\n",
    "# 将 tensor 变成 Variable\n",
    "x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度\n",
    "y = Variable(y_tensor, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = t.sum(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.2499)\n",
      "<SumBackward0 object at 0x00000000092E4A20>\n"
     ]
    }
   ],
   "source": [
    "print(z.data)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们打出了 z 中的 tensor 数值，同时通过 grad_fn 知道了其是通过 sum 这种方式得到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# 求 x 和 y 的梯度\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 .grad 我们得到了 x 和 y 的梯度，这里我们使用了 PyTorch 提供的自动求导机制，非常方便，下一节我们再具体讲解自动求导。\n",
    "\n",
    "#### 小练习\n",
    "\n",
    "尝试构建一个函数 $y = x^2$ ，然后求 x = 2 的导数。\n",
    "\n",
    "参考输出：4\n",
    "\n",
    "提示：\n",
    "\n",
    "$y = x^2$ 的图像如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x92e4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3, 3.01, 0.1)\n",
    "y = x ** 2\n",
    "plt.plot(x, y)\n",
    "plt.plot(2, 4, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.])\n"
     ]
    }
   ],
   "source": [
    "# 答案\n",
    "x = Variable(t.FloatTensor([2]), requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一次课程我们将会从导数展开，了解 PyTorch 的自动求导机制。\n",
    "\n",
    "#### 附加\n",
    "\n",
    "对于 backward 不理解的同学，看这个链接：https://l1aoxingyu.github.io/2017/07/10/backward/ 可以说写的简直不能再好了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
